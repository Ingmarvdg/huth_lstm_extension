{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for word representations in human brain tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "Language encoding models can help us understand the underlying mechanisms of language processing in the human brain. Alexander Huth explains how he uses a 985-dimensional word embedding to model FMRI response in [this tutorial](https://github.com/HuthLab/speechmodeltutorial). This word embedding method however, uses a quite outdated method. He also has a [follow-up paper](https://papers.nips.cc/paper/7897-incorporating-context-into-language-encoding-models-for-fmri) where he uses an LSTM to train the word representations to respond to context. This notebook is an extension of his own tutorial, where we use a more advanced word embedding method, as well as showing an example implementation of the LSTM, since his own tutorial does not contain it. At the end of the tutorial you will be left with a set of models that can be used to convert a text into word embeddings using a very similar method to Huth's. \n",
    "\n",
    "All the libraries needed in this tutorial, WordEmbedding and StoriesDataset are two custom classes to help loading the word embeddings and the dataset. LSTMWordpred is the PyTorch architecture for the LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and imports\n",
    "from nlp_fmri_utils import WordEmbedding, StoriesDataset, LSTMWordpred\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "modeldir = './models'\n",
    "seqdir = './seq_data'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will check if you have the proper datafiles, if not it will download them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to download the datafiles from the web\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"stories.csv\"): \n",
    "    r1 = requests.get('https://www.kaggle.com/shubchat/1002-short-stories-from-project-guttenberg/download', allow_redirects=True)\n",
    "    open('stories.csv', 'wb').write(r1.content)\n",
    "\n",
    "if not os.path.exists(\"glove_embeds.txt\"): \n",
    "    r2 = requests.get('http://nlp.stanford.edu/data/glove.6B.zip', allow_redirects=True)\n",
    "    open('glove.6B.zip', 'wb').write(r2.content)\n",
    "    \n",
    "    with zipfile.ZipFile('glove.6B.zip') as z:\n",
    "        with open('glove_embeds.txt', 'wb') as f:\n",
    "            f.write(z.read('glove.6B.50d.txt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: the word embedding\n",
    "To represent each word in the text, a pre-trained word embedding is used. The word embedding we use is a 50 dimensional GloVe embedding. We use a pre-trained word embedding because doing the embedding in this tutorial will be too time consuming. The GloVe embedding algorithm takes a sparse word co-occurrence matrix and turns it into a dense word embedding matrix. For more information on how this embedding works, check the [original paper](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "Initialize a WordEmbedding object with the path to the pre-trained embeddings, we will use this later to look at word similarities and to convert text to vectors. The pre-trained embeddings contain 40 thousand words with 50 dimensions each, so this might take a minute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made an embedding containing  400000 words.\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = WordEmbedding(f'glove_embeds.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the embeddings have been parsed correctly by seeing if the shape is correct. The array should be of shape n_words x n_dimensions. We can also test if we can find the vector of a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector matrix shape: (400000, 50)\n",
      "the word vector of the word cheese is:  [[-0.053903 -0.30871  -1.3285   -0.43342   0.31779   1.5224   -0.6965\n",
      "  -0.037086 -0.83784   0.074107 -0.30532  -0.1783    1.2337    0.085473\n",
      "   0.17362  -0.19001   0.36907   0.49454  -0.024311 -1.0535    0.5237\n",
      "  -1.1489    0.95093   1.1538   -0.52286  -0.14931  -0.97614   1.3912\n",
      "   0.79875  -0.72134   1.5411   -0.15928  -0.30472   1.7265    0.13124\n",
      "  -0.054023 -0.74212   1.675     1.9502   -0.53274   1.1359    0.20027\n",
      "   0.02245  -0.39379   1.0609    1.585     0.17889   0.43556   0.68161\n",
      "   0.066202]]\n"
     ]
    }
   ],
   "source": [
    "print(\"vector matrix shape:\", glove_embeddings.vectors.shape)\n",
    "test_word = 'cheese'\n",
    "print(f\"the word vector of the word {test_word} is: \", glove_embeddings.get_word_vectors([test_word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Below we use [TSNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), a method to cluster and visualize high dimensional data to show words of two distinct categories:\n",
    "1. food related items\n",
    "2. family related items\n",
    "\n",
    "TSNE is able to make a clear distinction between the two categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot words with tsne dimensionality reduction\n",
    "\n",
    "words = [\"mother\", \"mom\", \"dad\", \"father\", \"son\", \"family\", \"cheese\", \"pizza\", \"chicken\", \"tomato\", \"beef\", \"food\"]\n",
    "vects = glove_embeddings.get_word_vectors(words)\n",
    "\n",
    "viz_dims = TSNE(n_components=2, perplexity=5, random_state=0).fit_transform(vects)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "i=0\n",
    "\n",
    "for a in words:\n",
    "    ax.scatter(viz_dims[i][0], viz_dims[i][1], s=800, c='black', marker=r\"$ {} $\".format(a), edgecolors='none' )\n",
    "    i+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above we can conclude that the words within each cluster are somewhat similar, but how similar (or how different) are they exactly? We can use cosine similarity to quantify the similarity between two words, where 0 is most dissimilar and 1 is most similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between dad and mom is: 0.9040323212958359\n",
      "Similarity between dad and pizza is: 0.3952750348424894\n",
      "Similarity between pizza and cheese is: 0.7087556782372948\n",
      "Similarity between fruit and rock is: 0.328926165386697\n",
      "Similarity between man and woman is: 0.8860337718495819\n",
      "Similarity between food and family is: 0.5078556142593991\n"
     ]
    }
   ],
   "source": [
    "for pair in [('dad', 'mom'), \n",
    "             ('dad', 'pizza'), \n",
    "             ('pizza', 'cheese'), \n",
    "             ('fruit', 'rock'), \n",
    "             ('man', 'woman'),\n",
    "             ('food','family')\n",
    "             ]:\n",
    "    \n",
    "    print('Similarity between {} and {} is: {}'.\n",
    "          format(pair[0], pair[1], glove_embeddings.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go a step further and find the 5 most similar words from a given word, lets try some. Use the most_similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words most similar to soldier are: ['wounded', 'killed', 'soldiers', 'army', 'troops']\n",
      "The words most similar to king are: ['emperor', 'throne', 'son', 'lord', 'prince']\n",
      "The words most similar to food are: ['products', 'vegetables', 'meat', 'medicines', 'goods']\n",
      "The words most similar to rock are: ['band', 'album', 'music', 'songs', 'albums']\n",
      "The words most similar to family are: ['816-822-8448', 'mother', 'daughter', 'wife', 'son']\n"
     ]
    }
   ],
   "source": [
    "for word in ['soldier',\n",
    "             'king',\n",
    "             'food',\n",
    "             'rock',\n",
    "             'family'\n",
    "            ]:\n",
    "    \n",
    "    print('The words most similar to {} are: {}'.format(word, glove_embeddings.most_similar(word, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting emergence from these word embeddings is word analogies, for example a man is to a king what a woman is to a ?. The function takes the vector of a word, like \"man\", subtracts another vector, like \"king\", and then adds a third vector, like \"queen\". Then finds the nearest word to the resulting vector. We can try some word analogy triplets with the analogy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king is to man as queen is to ['woman', 'girl', 'her', 'man', 'she']\n",
      "university is to professor as school is to ['school', '816-822-8448', 'professor', 'college', 'teacher']\n",
      "mother is to daughter as father is to ['son', 'daughter', 'father', 'married', 'wife']\n",
      "god is to church as devil is to ['non-institutionalized', 'church', 'st.', 'cathedral', '732-390-4697']\n"
     ]
    }
   ],
   "source": [
    "triplets = [('king', 'man', 'queen'),\n",
    "            ('university', 'professor', 'school'),\n",
    "            ('mother', 'daughter', 'father'),\n",
    "            ('god', 'church', 'devil'),\n",
    "           ]\n",
    "\n",
    "for a, b, c in triplets:\n",
    "    print('{} is to {} as {} is to {}'.format(a, b, c, glove_embeddings.analogy(a, b, c, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analogy task is not performing perfectly, but the results are in the ballpark of what we could expect. Its clear that these word embeddings contain a lot of information, while only having 50 dimensions per word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: data preparation\n",
    "\n",
    "The texts used by Alexander Huth are short stories, so it only makes sense for us to use other short stories as training data for the LSTM. The texts we will use are from [PROJECT GUTENBERG](https://www.gutenberg.org/), it's a dataset with 1000 short stories. \n",
    "\n",
    "The StoriesDataset class will load the processed datafile if it exists, if not it must be generated first by using generate_sequences. If you dont have processed .csv files yet, run the cell below to create a processed dataset containing n stories. For the sake of this tutorial, 10 stories should be enough. \n",
    "\n",
    "The preprocessing the generate_sequences function does is the following:\n",
    "1. remove all non-word tokens and large whitespaces.\n",
    "2. lowercase all words.\n",
    "3. generate sequences of length n, with targets. \n",
    "\n",
    "The sequences will use the sliding window principle, with the last word being the target. \n",
    "\n",
    "Alexander Huths second paper uses 10 different context lengths to see if different parts of the brain react to which context length. For simplicity we will only use 3 context lengths, 4, 6 and 8. For this we must generate 3 separate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to generate sequences first if you havent already\n",
    "max_seq_len = 8\n",
    "context_lens = [4, 6, 8]\n",
    "n_stories = 2\n",
    "\n",
    "if not os.path.exists(seqdir):\n",
    "    os.mkdir(seqdir)\n",
    "\n",
    "for context_len in context_lens:\n",
    "    filename = 'seq_{}.csv'.format(context_len)\n",
    "    StoriesDataset(os.path.join(seqdir, filename) , glove_embeddings, max_seq_len).generate_sequences('stories.csv', 'content', n_stories, context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: training the LSTM\n",
    "\n",
    "Now that we have prepared the sequences and the targets, we can start training an LSTM model! The input of the model will be the word embeddings of a single sequence, the target output will be the index of the target word. \n",
    "\n",
    "The model will have the following layers:\n",
    "2. The first lstm layer, with an input shape of maximum sequence length * the word embedding size.\n",
    "3. The second lstm layer.\n",
    "4. The third lstm layer. \n",
    "5. A dense layer that with one output node for each unique word in the text, using a sigmoid activation.  \n",
    "\n",
    "The architecture of the model has already been made in PyTorch, we only have to call it and give it the right arguments, 3 LSTM layers of size 50.\n",
    "\n",
    "We will use cosine similarity (called CosineEmbeddingLoss in PyTorch) as loss function and adam as optimizer. We also have to retrieve the datasets we made earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "fit_params = {'n_epochs': 5,\n",
    "              'val_split': 0.1,\n",
    "              'batch_size': 64,\n",
    "              'loss_function': loss_fun}\n",
    "\n",
    "model_params = {'hidden_dim': 50,\n",
    "                'n_layers': 3,\n",
    "                'embedding_dim': glove_embeddings.dim}\n",
    "\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont forget that we have to make 3 models, one for each context length. Running the following cell will fit a model using the hyperparameters above for each context length we have available and save the model as a pickle file. \n",
    "\n",
    "Running this cell can take a long time if you dont have a strong GPU available for PyTorch to use. Luckily, I trained some models and saved them, if you dont want to wait for the training loop, use the pretrained models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model for seq_8.csv...\n",
      "Fitting model with 5 epochs over 12165 sequences.\n",
      "Starting epoch 1\n",
      "Training loss: 0.5802602986263674, Validation loss: 0.3387395786611657\n",
      "Starting epoch 2\n",
      "Training loss: 0.32262563150982526, Validation loss: 0.32923308485432673\n",
      "Starting epoch 3\n",
      "Training loss: 0.31557396322835324, Validation loss: 0.32380180374572154\n",
      "Starting epoch 4\n",
      "Training loss: 0.3116612242058266, Validation loss: 0.3197752276533528\n",
      "Starting epoch 5\n",
      "Training loss: 0.3089922252436017, Validation loss: 0.3188532708506835\n",
      "Making model for seq_6.csv...\n",
      "Fitting model with 5 epochs over 12169 sequences.\n",
      "Starting epoch 1\n",
      "Training loss: 0.5454651127720989, Validation loss: 0.32362320234901026\n",
      "Starting epoch 2\n",
      "Training loss: 0.3217649147954098, Validation loss: 0.3159543864036861\n",
      "Starting epoch 3\n",
      "Training loss: 0.3159680850803852, Validation loss: 0.31161562706294815\n",
      "Starting epoch 4\n",
      "Training loss: 0.31218224997783817, Validation loss: 0.3091312168460143\n",
      "Starting epoch 5\n",
      "Training loss: 0.3116697459373363, Validation loss: 0.30709983173169586\n",
      "Making model for seq_4.csv...\n",
      "Fitting model with 5 epochs over 12173 sequences.\n",
      "Starting epoch 1\n",
      "Training loss: 0.5892426804747692, Validation loss: 0.3663840010762215\n",
      "Starting epoch 2\n",
      "Training loss: 0.3261817213754321, Validation loss: 0.3433278948068619\n",
      "Starting epoch 3\n",
      "Training loss: 0.3186991531835046, Validation loss: 0.3386902630329132\n",
      "Starting epoch 4\n",
      "Training loss: 0.31476571038365364, Validation loss: 0.3320210374891758\n",
      "Starting epoch 5\n",
      "Training loss: 0.3111380620231462, Validation loss: 0.32540139779448507\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(modeldir):\n",
    "    os.mkdir(modeldir)\n",
    "\n",
    "for filename in os.listdir(seqdir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f'Making model for {filename}...')\n",
    "        model = LSTMWordpred(**model_params)\n",
    "        adam = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        dataset = StoriesDataset(filename, glove_embeddings)\n",
    "        \n",
    "        train_hist, val_hist = model.fit(dataset, opt=adam, **fit_params)\n",
    "        \n",
    "        filename = f'lstm_model{filename[-5]}.pth'\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(modeldir, filename))\n",
    "        \n",
    "        # add your code for loss history visualization here.\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will load the three pretrained models in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights of lstm_model8.pth into lstm_model8\n",
      "Loaded model weights of lstm_model4.pth into lstm_model4\n",
      "Loaded model weights of lstm_model6.pth into lstm_model6\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for filename in os.listdir(modeldir):\n",
    "    if filename.endswith(\".pth\"):\n",
    "        \n",
    "        model = LSTMWordpred(**model_params)\n",
    "        model.load_state_dict(torch.load(os.path.join(modeldir, filename)))\n",
    "        model.eval()\n",
    "        \n",
    "        modelname = filename[:-4]\n",
    "        \n",
    "        models[modelname] = model\n",
    "        \n",
    "        print(f'Loaded model weights of {filename} into {modelname}')\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sequence = glove_embeddings.get_word_vectors(['i', 'like', 'to', 'go'])\n",
    "\n",
    "example_sequence = torch.from_numpy(example_sequence).unsqueeze(dim=0).float()\n",
    "\n",
    "example = models['lstm_model4'].get_hidden_states(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been compiled, we can start fitting the sequences to the target. This will can take a VERY long time, depending on your machine, the amount of stories you have selected, and the amount of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is done training, we can inspect the performance over each epoch. We want to reach a point where the validation loss starts increasing or stays the same. If we go further we will begin to overfit, and we want to avoid that. When we tested it this was around 50 epochs, so for simplicity we will use this many epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we need a function that can extract the output of the 3rd lstm layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function where we can input a sequence and get its representation from the LSTM layer.\n",
    "\n",
    "In A. Huths paper different representation lengths are used so we will train 2 more models, one with context length 4, and one with context length 6. The code below will do what we did above for context length 2, but now for context length 4 and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we have our models. We can now test with a simple example and see how each context differs, based on the context length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
