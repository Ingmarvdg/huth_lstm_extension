{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for word representations in human brain tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "Language encoding models can help us understand the underlying mechanisms of language processing in the human brain. Alexander Huth explains how he uses a 985-dimensional word embedding to model FMRI response in [this tutorial](https://github.com/HuthLab/speechmodeltutorial). This word embedding method however, uses a quite outdated method. He also has a [follow-up paper](https://papers.nips.cc/paper/7897-incorporating-context-into-language-encoding-models-for-fmri) where he uses an LSTM to train the word representations to respond to context. This notebook is an extension of his own tutorial, where we use a more advanced word embedding method, as well as showing an example implementation of the LSTM, since his own tutorial does not contain it. At the end of the tutorial you will be left with a set of models that can be used to convert a text into word embeddings using a very similar method to Huth's. \n",
    "\n",
    "All the libraries needed in this tutorial, WordEmbedding and StoriesDataset are two custom classes to help loading the word embeddings and the dataset. LSTMWordpred is the PyTorch architecture for the LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "# configurations and imports\n",
    "from nlp_fmri_utils import WordEmbedding, StoriesDataset, LSTMWordpred, load_models\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "\n",
    "# functions from Alexander Huths tutorial\n",
    "import functions\n",
    "\n",
    "modeldir = './models'\n",
    "seqdir = './seq_data'\n",
    "gridsdir = './grids'\n",
    "trdir = './trfiles'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will check if you have the proper datafiles, if not it will download them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to download the datafiles from the web\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"stories.csv\"): \n",
    "    r1 = requests.get('https://www.kaggle.com/shubchat/1002-short-stories-from-project-guttenberg/download', allow_redirects=True)\n",
    "    open('stories.csv', 'wb').write(r1.content)\n",
    "\n",
    "if not os.path.exists(\"glove_embeds.txt\"): \n",
    "    r2 = requests.get('http://nlp.stanford.edu/data/glove.6B.zip', allow_redirects=True)\n",
    "    open('glove.6B.zip', 'wb').write(r2.content)\n",
    "    \n",
    "    with zipfile.ZipFile('glove.6B.zip') as z:\n",
    "        with open('glove_embeds.txt', 'wb') as f:\n",
    "            f.write(z.read('glove.6B.300d.txt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: the word embedding\n",
    "To represent each word in the text, a pre-trained word embedding is used. The word embedding we use is a 50 dimensional GloVe embedding. We use a pre-trained word embedding because doing the embedding in this tutorial will be too time consuming. The GloVe embedding algorithm takes a sparse word co-occurrence matrix and turns it into a dense word embedding matrix. For more information on how this embedding works, check the [original paper](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "Initialize a WordEmbedding object with the path to the pre-trained embeddings, we will use this later to look at word similarities and to convert text to vectors. The pre-trained embeddings contain 40 thousand words with 50 dimensions each, so this might take a minute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made an embedding containing  400000 words.\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = WordEmbedding(f'glove_embeds.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the embeddings have been parsed correctly by seeing if the shape is correct. The array should be of shape n_words x n_dimensions. We can also test if we can find the vector of a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector matrix shape: (400000, 50)\n",
      "the word vector of the word cheese is:  [[-0.053903 -0.30871  -1.3285   -0.43342   0.31779   1.5224   -0.6965\n",
      "  -0.037086 -0.83784   0.074107 -0.30532  -0.1783    1.2337    0.085473\n",
      "   0.17362  -0.19001   0.36907   0.49454  -0.024311 -1.0535    0.5237\n",
      "  -1.1489    0.95093   1.1538   -0.52286  -0.14931  -0.97614   1.3912\n",
      "   0.79875  -0.72134   1.5411   -0.15928  -0.30472   1.7265    0.13124\n",
      "  -0.054023 -0.74212   1.675     1.9502   -0.53274   1.1359    0.20027\n",
      "   0.02245  -0.39379   1.0609    1.585     0.17889   0.43556   0.68161\n",
      "   0.066202]]\n"
     ]
    }
   ],
   "source": [
    "print(\"vector matrix shape:\", glove_embeddings.vectors.shape)\n",
    "test_word = 'cheese'\n",
    "print(f\"the word vector of the word {test_word} is: \", glove_embeddings.get_word_vectors([test_word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Below we use [TSNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), a method to cluster and visualize high dimensional data to show words of two distinct categories:\n",
    "1. food related items\n",
    "2. family related items\n",
    "\n",
    "TSNE is able to make a clear distinction between the two categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot words with tsne dimensionality reduction\n",
    "\n",
    "words = [\"mother\", \"mom\", \"dad\", \"father\", \"son\", \"family\", \"cheese\", \"pizza\", \"chicken\", \"tomato\", \"beef\", \"food\"]\n",
    "vects = glove_embeddings.get_word_vectors(words)\n",
    "\n",
    "viz_dims = TSNE(n_components=2, perplexity=5, random_state=0).fit_transform(vects)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "i=0\n",
    "\n",
    "for a in words:\n",
    "    ax.scatter(viz_dims[i][0], viz_dims[i][1], s=800, c='black', marker=r\"$ {} $\".format(a), edgecolors='none' )\n",
    "    i+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above we can conclude that the words within each cluster are somewhat similar, but how similar (or how different) are they exactly? We can use cosine similarity to quantify the similarity between two words, where 0 is most dissimilar and 1 is most similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between dad and mom is: 0.9040323212958359\n",
      "Similarity between dad and pizza is: 0.3952750348424894\n",
      "Similarity between pizza and cheese is: 0.7087556782372948\n",
      "Similarity between fruit and rock is: 0.328926165386697\n",
      "Similarity between man and woman is: 0.8860337718495819\n",
      "Similarity between food and family is: 0.5078556142593991\n"
     ]
    }
   ],
   "source": [
    "for pair in [('dad', 'mom'), \n",
    "             ('dad', 'pizza'), \n",
    "             ('pizza', 'cheese'), \n",
    "             ('fruit', 'rock'), \n",
    "             ('man', 'woman'),\n",
    "             ('food','family')\n",
    "             ]:\n",
    "    \n",
    "    print('Similarity between {} and {} is: {}'.\n",
    "          format(pair[0], pair[1], glove_embeddings.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go a step further and find the 5 most similar words from a given word, lets try some. Use the most_similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words most similar to soldier are: ['wounded', 'killed', 'soldiers', 'army', 'troops']\n",
      "The words most similar to king are: ['emperor', 'throne', 'son', 'lord', 'prince']\n",
      "The words most similar to food are: ['products', 'vegetables', 'meat', 'medicines', 'goods']\n",
      "The words most similar to rock are: ['band', 'album', 'music', 'songs', 'albums']\n",
      "The words most similar to family are: ['816-822-8448', 'mother', 'daughter', 'wife', 'son']\n"
     ]
    }
   ],
   "source": [
    "for word in ['soldier',\n",
    "             'king',\n",
    "             'food',\n",
    "             'rock',\n",
    "             'family'\n",
    "            ]:\n",
    "    \n",
    "    print('The words most similar to {} are: {}'.format(word, glove_embeddings.most_similar(word, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting emergence from these word embeddings is word analogies, for example a man is to a king what a woman is to a ?. The function takes the vector of a word, like \"man\", subtracts another vector, like \"king\", and then adds a third vector, like \"queen\". Then finds the nearest word to the resulting vector. We can try some word analogy triplets with the analogy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king is to man as queen is to ['woman', 'girl', 'her', 'man', 'she']\n",
      "university is to professor as school is to ['school', '816-822-8448', 'professor', 'college', 'teacher']\n",
      "mother is to daughter as father is to ['son', 'daughter', 'father', 'married', 'wife']\n",
      "god is to church as devil is to ['non-institutionalized', 'church', 'st.', 'cathedral', '732-390-4697']\n"
     ]
    }
   ],
   "source": [
    "triplets = [('king', 'man', 'queen'),\n",
    "            ('university', 'professor', 'school'),\n",
    "            ('mother', 'daughter', 'father'),\n",
    "            ('god', 'church', 'devil'),\n",
    "           ]\n",
    "\n",
    "for a, b, c in triplets:\n",
    "    print('{} is to {} as {} is to {}'.format(a, b, c, glove_embeddings.analogy(a, b, c, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analogy task is not performing perfectly, but the results are in the ballpark of what we could expect. Its clear that these word embeddings contain a lot of information, while only having 50 dimensions per word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: data preparation\n",
    "\n",
    "The texts used by Alexander Huth are short stories, so it only makes sense for us to use other short stories as training data for the LSTM. The texts we will use are from [PROJECT GUTENBERG](https://www.gutenberg.org/), it's a dataset with 1000 short stories. \n",
    "\n",
    "The StoriesDataset class will load the processed datafile if it exists, if not it must be generated first by using generate_sequences. If you dont have processed .csv files yet, run the cell below to create a processed dataset containing n stories. For the sake of this tutorial, 10 stories should be enough. \n",
    "\n",
    "The preprocessing the generate_sequences function does is the following:\n",
    "1. remove all non-word tokens and large whitespaces.\n",
    "2. lowercase all words.\n",
    "3. generate sequences of length n, with targets. \n",
    "\n",
    "The sequences will use the sliding window principle, with the last word being the target. \n",
    "\n",
    "Alexander Huths second paper uses 10 different context lengths to see if different parts of the brain react to which context length. For simplicity we will only use 3 context lengths, 5, 10 and 15. For this we must generate 3 separate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to generate sequences first if you havent already\n",
    "context_lens = [5, 10, 15]\n",
    "n_stories = 3\n",
    "\n",
    "if not os.path.exists(seqdir):\n",
    "    os.mkdir(seqdir)\n",
    "\n",
    "for context_len in context_lens:\n",
    "    filename = 'seq_{}.csv'.format(context_len)\n",
    "    StoriesDataset(os.path.join(seqdir, filename) , glove_embeddings).generate_sequences('stories.csv', 'content', n_stories, context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: training the LSTM\n",
    "\n",
    "Now that we have prepared the sequences and the targets, we can start training an LSTM model! The input of the model will be the word embeddings of a single sequence, the target output will be the index of the target word. \n",
    "\n",
    "The model will have the following layers:\n",
    "2. The first lstm layer, with an input shape of maximum sequence length * the word embedding size.\n",
    "3. The second lstm layer.\n",
    "4. The third lstm layer. \n",
    "5. A dense layer that with one output node for each unique word in the text, using a sigmoid activation.  \n",
    "\n",
    "The architecture of the model has already been made in PyTorch, we only have to call it and give it the right arguments, 3 LSTM layers of size 50.\n",
    "\n",
    "We will use cosine similarity (called CosineEmbeddingLoss in PyTorch) as loss function and adam as optimizer. We also have to retrieve the datasets we made earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = torch.nn.MSELoss()\n",
    "\n",
    "fit_params = {'n_epochs': 3,\n",
    "              'val_split': 0.1,\n",
    "              'batch_size': 10,\n",
    "              'loss_function': loss_fun}\n",
    "\n",
    "model_params = {'hidden_dim': 50,\n",
    "                'n_layers': 3,\n",
    "                'embedding_dim': glove_embeddings.dim}\n",
    "\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont forget that we have to make 3 models, one for each context length. Running the following cell will fit a model using the hyperparameters above for each context length we have available and save the model as a pickle file. \n",
    "\n",
    "Running this cell can take a long time if you dont have a strong GPU available for PyTorch to use. Luckily, I trained some models and saved them, if you dont want to wait for the training loop, use the pretrained models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model for seq_15.csv...\n",
      "seq_15.csv\n",
      "Fitting model with 3 epochs over 57495 sequences.\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-64f94ed7c528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seq_(.+?).csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'lstm_model{cl}.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Education/Exchange/AFMRIA/nlp_fmri_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, n_epochs, val_split, batch_size, loss_function, opt, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m# print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_tar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(modeldir):\n",
    "    os.mkdir(modeldir)\n",
    "\n",
    "for filename in os.listdir(seqdir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f'Making model for {filename}...')\n",
    "        model = LSTMWordpred(**model_params)\n",
    "        adam = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        print(filename)\n",
    "        ds = StoriesDataset(os.path.join(seqdir, filename), glove_embeddings)\n",
    "        \n",
    "        cl = re.search('seq_(.+?).csv', filename).group(1)\n",
    "        \n",
    "        train_hist, val_hist = model.fit(ds, opt=adam, **fit_params)\n",
    "        filename = f'lstm_model{cl}.pth'\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(modeldir, filename))\n",
    "        \n",
    "        # add your code for loss history visualization here.\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will load the pretrained models in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch = LSTMWordpred(**model_params)\n",
    "\n",
    "models = load_models(model_arch, modeldir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the new representations we need a replacement for the make_semantic_model function used by Huth, this function is the one below. This repo does not contain the Datasequence class that the original paper uses, so it will give an error when you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function to make the semantic model\n",
    "from functions import DataSequence\n",
    "def make_semantic_lstm_model(ds, embeddings, lstm_model, layer, seq_len):\n",
    "    x = []\n",
    "    words = ds.data\n",
    "    for i in range(0, len(words), 1):\n",
    "        if i <= seq_len:\n",
    "            seq = list(seq_len*' ')\n",
    "        else:\n",
    "            seq = words[i-seq_len:i]\n",
    "        \n",
    "        seq_emb = embeddings.get_word_vectors(seq)\n",
    "        x.append(seq_emb)\n",
    "    seqs = torch.from_numpy(np.stack(x)).float()\n",
    "    representations = lstm_model.get_hidden_states(seqs)[layer-1].detach().numpy()\n",
    "    \n",
    "    return DataSequence(representations, ds.split_inds, ds.data_times, ds.tr_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it. To use the LSTM for word representations instead of the Eng1000 representation, you will have to make the following adjustments to the original tutorial:\n",
    "\n",
    "1. load the glove word embeddings (cell 3).\n",
    "2. load the pretrained models (cell 10 and 11).\n",
    "3. copy the new semantic model function (cell 13).\n",
    "4. use the lstm semantic model function instead of the original function.\n",
    "\n",
    "The files and directories you will need are:\n",
    "1. A models folder with the pretrained models.\n",
    "2. The word embeddings .txt file.\n",
    "3. The nlp_utils.py file\n",
    "\n",
    "I might improve on this explanation by uploading the modified tutorial, or a more detailed walkthrough. But for now, this will have to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE NEXT PART IS COPIED FROM HUTHS TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are lists of the stories\n",
    "# Rstories are the names of the training (or Regression) stories, which we will use to fit our models\n",
    "Rstories = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "            'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "            'odetostepfather', 'souls', 'undertheinfluence']\n",
    "\n",
    "# Pstories are the test (or Prediction) stories (well, story), which we will use to test our models\n",
    "Pstories = ['wheretheressmoke']\n",
    "\n",
    "allstories = Rstories + Pstories\n",
    "\n",
    "# Load TextGrids\n",
    "from functions import load_grids_for_stories\n",
    "grids = load_grids_for_stories(allstories, gridsdir)\n",
    "\n",
    "# Load TRfiles\n",
    "from functions import load_generic_trfiles\n",
    "trfiles = load_generic_trfiles(allstories, trdir)\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "from functions import make_word_ds, make_phoneme_ds\n",
    "wordseqs = make_word_ds(grids, trfiles) # dictionary of {storyname : word DataSequence}\n",
    "phonseqs = make_phoneme_ds(grids, trfiles) # dictionary of {storyname : phoneme DataSequence}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project stimuli\n",
    "from functions import make_semantic_model\n",
    "semanticseqs = dict() # dictionary to hold projected stimuli {story name : projected DataSequence}\n",
    "for story in allstories:\n",
    "    semanticseqs[story] = make_semantic_lstm_model(wordseqs[story], glove_embeddings, models['lstm_model10'], 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = semanticseqs['naked'].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample stimuli\n",
    "interptype = \"lanczos\" # filter type\n",
    "window = 3 # number of lobes in Lanczos filter\n",
    "\n",
    "downsampled_semanticseqs = dict() # dictionary to hold downsampled stimuli\n",
    "for story in allstories:\n",
    "    downsampled_semanticseqs[story] = semanticseqs[story].chunksums(interptype, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "s_words = wordseqs[\"naked\"]\n",
    "s_sem = semanticseqs[\"naked\"]\n",
    "s_semdown = downsampled_semanticseqs[\"naked\"]\n",
    "\n",
    "plt.plot(s_sem.data[10,:])\n",
    "plt.plot(s_sem.data[21,:])\n",
    "plt.plot(s_sem.data[32,:])\n",
    "plt.plot(s_sem.data[43,:])\n",
    "plt.plot(s_sem.data[74,:])\n",
    "\n",
    "print(s_sem.data[:,10].std())\n",
    "print(s_sem.data[:,10].mean())\n",
    "print(s_semdown.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5), facecolor='w')\n",
    "f.clf()\n",
    "schan = 6\n",
    "ax = f.add_subplot(1,1,1)\n",
    "wordstems = ax.stem(s_sem.data_times,\n",
    "                    s_sem.data[:,schan] / np.abs(s_sem.data[:,schan]).max(),\n",
    "                    linefmt=\"k-\", markerfmt=\"k.\", basefmt=\"k-\", use_line_collection=True)\n",
    "interps = ax.plot(s_sem.tr_times,\n",
    "                  s_semdown[:,schan] / np.abs(s_semdown[:,schan]).max(), 'r.-')\n",
    "ax.set_xlim(-6, 60)\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_xlabel(\"Time (seconds since story start)\")\n",
    "ax.set_ylabel(\"Semantic feature value\")\n",
    "ax.legend((wordstems, interps[0]), (\"Individual words\", \"Downsampled feature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
